{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.distributions.one_hot_categorical import OneHotCategorical\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "from torch.nn.parameter import Parameter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.4.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = './data'\n",
    "if not os.path.exists(root):\n",
    "    os.mkdir(root)\n",
    "    \n",
    "trans = transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])\n",
    "#transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))])\n",
    "train_set =  datasets.MNIST('../data', train=True, download=True, transform=trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = datasets.MNIST('../data', train=False, transform=trans, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "                 dataset=train_set,\n",
    "                 batch_size=batch_size,\n",
    "                 shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "                dataset=test_set,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.eye(2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvolution(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        input = input.cuda()\n",
    "        adj = adj.cuda()\n",
    "        #print(input.shape)\n",
    "        #print(self.weight.shape)\n",
    "        support = torch.matmul(input, self.weight)\n",
    "        output = torch.matmul(adj, support)\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "        \n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout):\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        self.gc1 = GraphConvolution(nfeat, nhid).to(device).type(torch.float)\n",
    "        self.gc2 = GraphConvolution(nhid, nclass).to(device).type(torch.float)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.relu(self.gc1(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = self.gc2(x, adj)\n",
    "        return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class graph_sample(nn.Module):\n",
    "    def __init__(self, input_shape, output_size, output=10,num_features = 1,  hidden_graph=100, M=10, num_sample=20, dropout=0.2, batch_size=100):\n",
    "        super(graph_sample, self).__init__()\n",
    "        #There are M bivariate gaussian: Each bivariate gaussian has 5 parameters  + vector M which contains the mixture weight  \n",
    "        self.m_g_params = Parameter(torch.randn(batch_size, 6*M).to(device))\n",
    "        self.batch_size = batch_size\n",
    "        self.num_sample = num_sample\n",
    "        self.gcn = GCN(num_features, hidden_graph, output, dropout).to(device).type(torch.float)\n",
    "        self.max_pooling = nn.MaxPool1d(20)\n",
    "        self.fcf = nn.Linear(output*num_sample,output)\n",
    "        self.output_gcn = output\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.params = torch.split(self.m_g_params, 6, 1)\n",
    "        #print(len(self.params))\n",
    "        self.params_mixture = torch.stack(self.params)\n",
    "        self.pi,self.mu_x,self.mu_y,self.sigma_x,self.sigma_y,self.rho_xy = torch.split(self.params_mixture,1,2)\n",
    "        self.pi = self.pi.squeeze().view(self.batch_size,-1)\n",
    "        self.pi = F.softmax(self.pi,dim=-1)\n",
    "        self.mu_x = self.mu_x.squeeze().view(self.batch_size,-1)\n",
    "        self.mu_y = self.mu_y.squeeze().view(self.batch_size,-1)\n",
    "        self.sigma_x =  self.sigma_x.squeeze().view(self.batch_size,-1)\n",
    "        self.sigma_y =  self.sigma_y.squeeze().view(self.batch_size,-1)\n",
    "        self.rho_xy =  self.rho_xy.squeeze().view(self.batch_size,-1)\n",
    "        samples = []\n",
    "        for i in range(self.num_sample):    \n",
    "            loc = self.sample_points()\n",
    "            pixel_value = self.get_pixel_value(x,loc) #currently not doing bilinear interpolation \n",
    "            samples.append(pixel_value)\n",
    "            #break\n",
    "        output = torch.stack(samples)\n",
    "        feature_matrix = output.view(self.batch_size,self.num_sample, -1)\n",
    "        adject_matrix = torch.ones(self.num_sample,self.num_sample)\n",
    "        output = self.gcn(feature_matrix,adject_matrix)\n",
    "        output = output.view(self.batch_size,  self.output_gcn*self.num_sample)\n",
    "        output = F.softmax(self.fcf(output), dim=-1)\n",
    "        #output = self.max_pooling(output)\n",
    "        return output.squeeze(-1)\n",
    "    \n",
    "    def sample_points(self):\n",
    "        #can do reparametrization trick: https://arxiv.org/abs/1607.05690\n",
    "        m = Categorical(self.pi)\n",
    "        pi_idx = m.sample().to(device)\n",
    "        pi_idx = pi_idx.view(-1,1)\n",
    "        mu_x = torch.gather(self.mu_x,1,pi_idx)\n",
    "        mu_y = torch.gather(self.mu_y,1,pi_idx)\n",
    "        sigma_x = torch.exp(torch.gather(self.sigma_x,1,pi_idx))\n",
    "        sigma_y = torch.exp(torch.gather(self.sigma_y,1,pi_idx))\n",
    "        rho_xy = torch.tanh(torch.gather(self.rho_xy,1,pi_idx))\n",
    "        loc =  self.sample_bivariate_normal(mu_x,mu_y,sigma_x,sigma_y,rho_xy, greedy=False)\n",
    "        return loc\n",
    "    \n",
    "    def sample_bivariate_normal(self, mu_x,mu_y,sigma_x,sigma_y,rho_xy, greedy=False):\n",
    "        if greedy:\n",
    "            return mu_x,mu_y\n",
    "        mean = torch.stack((mu_x, mu_y), dim=-1).squeeze()\n",
    "        cov_axis0 = torch.stack([sigma_x * sigma_x, rho_xy * sigma_x * sigma_y], dim=-1).squeeze()\n",
    "        cov_axis1 = torch.stack([rho_xy * sigma_x * sigma_y, sigma_y * sigma_y], dim=-1).squeeze()\n",
    "        cov = torch.stack((cov_axis0,cov_axis1), dim=-1)\n",
    "        m = MultivariateNormal(mean, cov)\n",
    "        x = m.sample()\n",
    "        return F.tanh(x) #Normalize it to -1 to 1 to get pixel value\n",
    "    \n",
    "    def get_pixel_value(self,x,loc):\n",
    "        B, C, H, W = x.shape\n",
    "        denorm_loc = self.denormalize(H, loc) #height and width are same \n",
    "        #denorm_loc =torch.split(denorm_loc.unsqueeze(1),1,2)\n",
    "        pixel_features = []\n",
    "        for i,image in enumerate(x):\n",
    "            pixel_value = image[:,denorm_loc[i][0],denorm_loc[i][1]]\n",
    "            pixel_features.append(pixel_value)\n",
    "        pixel_features = torch.stack(pixel_features)\n",
    "        #print(pixel_features.shape)\n",
    "        return pixel_features\n",
    "        \n",
    "    def denormalize(self, T, coords):\n",
    "        return (0.5 * ((coords + 1.0) * T) - 0.1).long()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (28,28)\n",
    "output_shape = (1,10)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "gs = graph_sample(input_shape, output_shape, batch_size = batch_size).to(device).type(torch.float)\n",
    "# gs.forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(gs.parameters(), lr=0.00003)\n",
    "num_epoch=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adityasan92/anaconda2/envs/pytorch/lib/python3.5/site-packages/ipykernel_launcher.py:47: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 batch: 1 Reward Avg:  95.5\n",
      "Epoch: 0 batch: 51 Reward Avg:  98.13461538461539\n",
      "Epoch0loss:  98.71666666666667\n",
      "Epoch: 1 batch: 1 Reward Avg:  100.0\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epoch):\n",
    "    loss_arr = []\n",
    "    reward_arr = []\n",
    "    for batch_idx, (data, label) in enumerate(train_loader):\n",
    "        label = label.to(device).type(torch.cuda.LongTensor)\n",
    "        pred_output = gs.forward(data.to(device).type(torch.float))\n",
    "        values, indices = torch.max(pred_output, 1)\n",
    "        reward = torch.sum(indices == label)\n",
    "        # Note that this is equivalent to what used to be called multinomial\n",
    "        m = Categorical(pred_output)\n",
    "        action = m.sample()\n",
    "        loss = -m.log_prob(action) * reward.type(torch.cuda.FloatTensor)\n",
    "        loss = loss.sum()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_arr.append(loss)\n",
    "        reward_arr.append(reward)\n",
    "        if(batch_idx % 50 == True):\n",
    "            print(\"Epoch: \" + str(epoch) + \" batch: \" + str(batch_idx) + \" Reward Avg: \",  np.mean(torch.stack(reward_arr).cpu().detach().numpy()))\n",
    "            self.m_g_params\n",
    "        #break\n",
    "    print(\"Epoch\" +  str(epoch) + \"loss: \", np.mean(torch.stack(reward_arr).cpu().detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize( T, coords):\n",
    "    return (0.5 * ((coords + 1.0) * T) - 0.1).long()\n",
    "denormalize(torch.tensor(1),28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
