{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.distributions.one_hot_categorical import OneHotCategorical\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "from torch.nn.parameter import Parameter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.4.0'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = './data'\n",
    "if not os.path.exists(root):\n",
    "    os.mkdir(root)\n",
    "    \n",
    "trans = transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])\n",
    "#transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))])\n",
    "train_set =  datasets.MNIST('../data', train=True, download=True, transform=trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = datasets.MNIST('../data', train=False, transform=trans, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "                 dataset=train_set,\n",
    "                 batch_size=batch_size,\n",
    "                 shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "                dataset=test_set,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15000"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.eye(2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvolution(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        input = input.cuda()\n",
    "        adj = adj.cuda()\n",
    "        #print(input.shape)\n",
    "        #print(self.weight.shape)\n",
    "        support = torch.matmul(input, self.weight)\n",
    "        output = torch.matmul(adj, support)\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "        \n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout):\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        self.gc1 = GraphConvolution(nfeat, nhid).to(device).type(torch.float)\n",
    "        self.gc2 = GraphConvolution(nhid, nclass).to(device).type(torch.float)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.relu(self.gc1(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = self.gc2(x, adj)\n",
    "        return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class graph_sample(nn.Module):\n",
    "    def __init__(self, input_shape, output_size, output=10,num_features = 1,  hidden_graph=100, M=10, num_sample=20, dropout=0.2, batch_size=4):\n",
    "        super(graph_sample, self).__init__()\n",
    "        #There are M bivariate gaussian: Each bivariate gaussian has 5 parameters  + vector M which contains the mixture weight  \n",
    "        self.m_g_params = Parameter(torch.randn(batch_size, 6*M).to(device))\n",
    "        self.batch_size = batch_size\n",
    "        self.num_sample = num_sample\n",
    "        self.gcn = GCN(num_features, hidden_graph, output, dropout).to(device).type(torch.float)\n",
    "        self.max_pooling = nn.MaxPool1d(20)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.params = torch.split(self.m_g_params, 6, 1)\n",
    "        #print(len(self.params))\n",
    "        self.params_mixture = torch.stack(self.params)\n",
    "        self.pi,self.mu_x,self.mu_y,self.sigma_x,self.sigma_y,self.rho_xy = torch.split(self.params_mixture,1,2)\n",
    "        self.pi = self.pi.squeeze().view(self.batch_size,-1)\n",
    "        self.pi = F.softmax(self.pi,dim=-1)\n",
    "        self.mu_x = self.mu_x.squeeze().view(self.batch_size,-1)\n",
    "        self.mu_y = self.mu_y.squeeze().view(self.batch_size,-1)\n",
    "        self.sigma_x =  self.sigma_x.squeeze().view(self.batch_size,-1)\n",
    "        self.sigma_y =  self.sigma_y.squeeze().view(self.batch_size,-1)\n",
    "        self.rho_xy =  self.rho_xy.squeeze().view(self.batch_size,-1)\n",
    "        samples = []\n",
    "        for i in range(self.num_sample):    \n",
    "            loc = self.sample_points()\n",
    "            pixel_value = self.get_pixel_value(x,loc) #currently not doing bilinear interpolation \n",
    "            samples.append(pixel_value)\n",
    "            #break\n",
    "        output = torch.stack(samples)\n",
    "        feature_matrix = output.view(self.batch_size,self.num_sample, -1)\n",
    "        adject_matrix = torch.ones(self.num_sample,self.num_sample)\n",
    "        output = self.gcn(feature_matrix,adject_matrix)\n",
    "        output = output.view(self.batch_size, -1, self.num_sample)\n",
    "        print(output)\n",
    "        output = self.max_pooling(output)\n",
    "        return output.squeeze(-1)\n",
    "    \n",
    "    def sample_points(self):\n",
    "        #can do reparametrization trick: https://arxiv.org/abs/1607.05690\n",
    "        m = Categorical(self.pi)\n",
    "        pi_idx = m.sample().to(device)\n",
    "        pi_idx = pi_idx.view(-1,1)\n",
    "        mu_x = torch.gather(self.mu_x,1,pi_idx)\n",
    "        mu_y = torch.gather(self.mu_y,1,pi_idx)\n",
    "        sigma_x = torch.exp(torch.gather(self.sigma_x,1,pi_idx))\n",
    "        sigma_y = torch.exp(torch.gather(self.sigma_y,1,pi_idx))\n",
    "        rho_xy = torch.tanh(torch.gather(self.rho_xy,1,pi_idx))\n",
    "        loc =  self.sample_bivariate_normal(mu_x,mu_y,sigma_x,sigma_y,rho_xy, greedy=False)\n",
    "        return loc\n",
    "    \n",
    "    def sample_bivariate_normal(self, mu_x,mu_y,sigma_x,sigma_y,rho_xy, greedy=False):\n",
    "        if greedy:\n",
    "            return mu_x,mu_y\n",
    "        mean = torch.stack((mu_x, mu_y), dim=-1).squeeze()\n",
    "        cov_axis0 = torch.stack([sigma_x * sigma_x, rho_xy * sigma_x * sigma_y], dim=-1).squeeze()\n",
    "        cov_axis1 = torch.stack([rho_xy * sigma_x * sigma_y, sigma_y * sigma_y], dim=-1).squeeze()\n",
    "        cov = torch.stack((cov_axis0,cov_axis1), dim=-1)\n",
    "        m = MultivariateNormal(mean, cov)\n",
    "        x = m.sample()\n",
    "        return F.tanh(x) #Normalize it to -1 to 1 to get pixel value\n",
    "    \n",
    "    def get_pixel_value(self,x,loc):\n",
    "        B, C, H, W = x.shape\n",
    "        denorm_loc = self.denormalize(H, loc) #height and width are same \n",
    "        #denorm_loc =torch.split(denorm_loc.unsqueeze(1),1,2)\n",
    "        pixel_features = []\n",
    "        for i,image in enumerate(x):\n",
    "            #print(denorm_loc[i][0],denorm_loc[i][1])\n",
    "            pixel_value = image[:,denorm_loc[i][0],denorm_loc[i][1]]\n",
    "            pixel_features.append(pixel_value)\n",
    "        pixel_features = torch.stack(pixel_features)\n",
    "        #print(pixel_features.shape)\n",
    "        return pixel_features\n",
    "        \n",
    "    def denormalize(self, T, coords):\n",
    "        return (0.5 * ((coords + 1.0) * T) - 0.1).long()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (28,28)\n",
    "output_shape = (1,10)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "gs = graph_sample(input_shape, output_shape).to(device).type(torch.float)\n",
    "# gs.forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 20, 1])\n",
      "torch.Size([1, 100])\n",
      "torch.Size([4, 20, 100])\n",
      "torch.Size([100, 10])\n",
      "tensor([[[-4.1772e+00, -2.4494e+00, -1.2382e+01, -3.5729e+00, -4.9893e+00,\n",
      "          -1.3133e+00, -6.0436e+00, -9.1157e+00, -8.9274e+00, -5.0473e+00,\n",
      "          -4.1772e+00, -2.4494e+00, -1.2382e+01, -3.5729e+00, -4.9893e+00,\n",
      "          -1.3133e+00, -6.0436e+00, -9.1157e+00, -8.9274e+00, -5.0473e+00],\n",
      "         [-4.1772e+00, -2.4494e+00, -1.2382e+01, -3.5729e+00, -4.9893e+00,\n",
      "          -1.3133e+00, -6.0436e+00, -9.1157e+00, -8.9274e+00, -5.0473e+00,\n",
      "          -4.1772e+00, -2.4494e+00, -1.2382e+01, -3.5729e+00, -4.9893e+00,\n",
      "          -1.3133e+00, -6.0436e+00, -9.1157e+00, -8.9274e+00, -5.0473e+00],\n",
      "         [-4.1772e+00, -2.4494e+00, -1.2382e+01, -3.5729e+00, -4.9893e+00,\n",
      "          -1.3133e+00, -6.0436e+00, -9.1157e+00, -8.9274e+00, -5.0473e+00,\n",
      "          -4.1772e+00, -2.4494e+00, -1.2382e+01, -3.5729e+00, -4.9893e+00,\n",
      "          -1.3133e+00, -6.0436e+00, -9.1157e+00, -8.9274e+00, -5.0473e+00],\n",
      "         [-4.1772e+00, -2.4494e+00, -1.2382e+01, -3.5729e+00, -4.9893e+00,\n",
      "          -1.3133e+00, -6.0436e+00, -9.1157e+00, -8.9274e+00, -5.0473e+00,\n",
      "          -4.1772e+00, -2.4494e+00, -1.2382e+01, -3.5729e+00, -4.9893e+00,\n",
      "          -1.3133e+00, -6.0436e+00, -9.1157e+00, -8.9274e+00, -5.0473e+00],\n",
      "         [-4.1772e+00, -2.4494e+00, -1.2382e+01, -3.5729e+00, -4.9893e+00,\n",
      "          -1.3133e+00, -6.0436e+00, -9.1157e+00, -8.9274e+00, -5.0473e+00,\n",
      "          -4.1772e+00, -2.4494e+00, -1.2382e+01, -3.5729e+00, -4.9893e+00,\n",
      "          -1.3133e+00, -6.0436e+00, -9.1157e+00, -8.9274e+00, -5.0473e+00],\n",
      "         [-4.1772e+00, -2.4494e+00, -1.2382e+01, -3.5729e+00, -4.9893e+00,\n",
      "          -1.3133e+00, -6.0436e+00, -9.1157e+00, -8.9274e+00, -5.0473e+00,\n",
      "          -4.1772e+00, -2.4494e+00, -1.2382e+01, -3.5729e+00, -4.9893e+00,\n",
      "          -1.3133e+00, -6.0436e+00, -9.1157e+00, -8.9274e+00, -5.0473e+00],\n",
      "         [-4.1772e+00, -2.4494e+00, -1.2382e+01, -3.5729e+00, -4.9893e+00,\n",
      "          -1.3133e+00, -6.0436e+00, -9.1157e+00, -8.9274e+00, -5.0473e+00,\n",
      "          -4.1772e+00, -2.4494e+00, -1.2382e+01, -3.5729e+00, -4.9893e+00,\n",
      "          -1.3133e+00, -6.0436e+00, -9.1157e+00, -8.9274e+00, -5.0473e+00],\n",
      "         [-4.1772e+00, -2.4494e+00, -1.2382e+01, -3.5729e+00, -4.9893e+00,\n",
      "          -1.3133e+00, -6.0436e+00, -9.1157e+00, -8.9274e+00, -5.0473e+00,\n",
      "          -4.1772e+00, -2.4494e+00, -1.2382e+01, -3.5729e+00, -4.9893e+00,\n",
      "          -1.3133e+00, -6.0436e+00, -9.1157e+00, -8.9274e+00, -5.0473e+00],\n",
      "         [-4.1772e+00, -2.4494e+00, -1.2382e+01, -3.5729e+00, -4.9893e+00,\n",
      "          -1.3133e+00, -6.0436e+00, -9.1157e+00, -8.9274e+00, -5.0473e+00,\n",
      "          -4.1772e+00, -2.4494e+00, -1.2382e+01, -3.5729e+00, -4.9893e+00,\n",
      "          -1.3133e+00, -6.0436e+00, -9.1157e+00, -8.9274e+00, -5.0473e+00],\n",
      "         [-4.1772e+00, -2.4494e+00, -1.2382e+01, -3.5729e+00, -4.9893e+00,\n",
      "          -1.3133e+00, -6.0436e+00, -9.1157e+00, -8.9274e+00, -5.0473e+00,\n",
      "          -4.1772e+00, -2.4494e+00, -1.2382e+01, -3.5729e+00, -4.9893e+00,\n",
      "          -1.3133e+00, -6.0436e+00, -9.1157e+00, -8.9274e+00, -5.0473e+00]],\n",
      "\n",
      "        [[-3.0707e+00, -3.3914e+00, -8.5459e+00, -4.2488e+00, -1.9966e+00,\n",
      "          -1.6208e+00, -6.3082e+00, -8.2080e+00, -9.3880e+00, -6.4663e+00,\n",
      "          -3.0707e+00, -3.3914e+00, -8.5459e+00, -4.2488e+00, -1.9966e+00,\n",
      "          -1.6208e+00, -6.3082e+00, -8.2080e+00, -9.3880e+00, -6.4663e+00],\n",
      "         [-3.0707e+00, -3.3914e+00, -8.5459e+00, -4.2488e+00, -1.9966e+00,\n",
      "          -1.6208e+00, -6.3082e+00, -8.2080e+00, -9.3880e+00, -6.4663e+00,\n",
      "          -3.0707e+00, -3.3914e+00, -8.5459e+00, -4.2488e+00, -1.9966e+00,\n",
      "          -1.6208e+00, -6.3082e+00, -8.2080e+00, -9.3880e+00, -6.4663e+00],\n",
      "         [-3.0707e+00, -3.3914e+00, -8.5459e+00, -4.2488e+00, -1.9966e+00,\n",
      "          -1.6208e+00, -6.3082e+00, -8.2080e+00, -9.3880e+00, -6.4663e+00,\n",
      "          -3.0707e+00, -3.3914e+00, -8.5459e+00, -4.2488e+00, -1.9966e+00,\n",
      "          -1.6208e+00, -6.3082e+00, -8.2080e+00, -9.3880e+00, -6.4663e+00],\n",
      "         [-3.0707e+00, -3.3914e+00, -8.5459e+00, -4.2488e+00, -1.9966e+00,\n",
      "          -1.6208e+00, -6.3082e+00, -8.2080e+00, -9.3880e+00, -6.4663e+00,\n",
      "          -3.0707e+00, -3.3914e+00, -8.5459e+00, -4.2488e+00, -1.9966e+00,\n",
      "          -1.6208e+00, -6.3082e+00, -8.2080e+00, -9.3880e+00, -6.4663e+00],\n",
      "         [-3.0707e+00, -3.3914e+00, -8.5459e+00, -4.2488e+00, -1.9966e+00,\n",
      "          -1.6208e+00, -6.3082e+00, -8.2080e+00, -9.3880e+00, -6.4663e+00,\n",
      "          -3.0707e+00, -3.3914e+00, -8.5459e+00, -4.2488e+00, -1.9966e+00,\n",
      "          -1.6208e+00, -6.3082e+00, -8.2080e+00, -9.3880e+00, -6.4663e+00],\n",
      "         [-3.0707e+00, -3.3914e+00, -8.5459e+00, -4.2488e+00, -1.9966e+00,\n",
      "          -1.6208e+00, -6.3082e+00, -8.2080e+00, -9.3880e+00, -6.4663e+00,\n",
      "          -3.0707e+00, -3.3914e+00, -8.5459e+00, -4.2488e+00, -1.9966e+00,\n",
      "          -1.6208e+00, -6.3082e+00, -8.2080e+00, -9.3880e+00, -6.4663e+00],\n",
      "         [-3.0707e+00, -3.3914e+00, -8.5459e+00, -4.2488e+00, -1.9966e+00,\n",
      "          -1.6208e+00, -6.3082e+00, -8.2080e+00, -9.3880e+00, -6.4663e+00,\n",
      "          -3.0707e+00, -3.3914e+00, -8.5459e+00, -4.2488e+00, -1.9966e+00,\n",
      "          -1.6208e+00, -6.3082e+00, -8.2080e+00, -9.3880e+00, -6.4663e+00],\n",
      "         [-3.0707e+00, -3.3914e+00, -8.5459e+00, -4.2488e+00, -1.9966e+00,\n",
      "          -1.6208e+00, -6.3082e+00, -8.2080e+00, -9.3880e+00, -6.4663e+00,\n",
      "          -3.0707e+00, -3.3914e+00, -8.5459e+00, -4.2488e+00, -1.9966e+00,\n",
      "          -1.6208e+00, -6.3082e+00, -8.2080e+00, -9.3880e+00, -6.4663e+00],\n",
      "         [-3.0707e+00, -3.3914e+00, -8.5459e+00, -4.2488e+00, -1.9966e+00,\n",
      "          -1.6208e+00, -6.3082e+00, -8.2080e+00, -9.3880e+00, -6.4663e+00,\n",
      "          -3.0707e+00, -3.3914e+00, -8.5459e+00, -4.2488e+00, -1.9966e+00,\n",
      "          -1.6208e+00, -6.3082e+00, -8.2080e+00, -9.3880e+00, -6.4663e+00],\n",
      "         [-3.0707e+00, -3.3914e+00, -8.5459e+00, -4.2488e+00, -1.9966e+00,\n",
      "          -1.6208e+00, -6.3082e+00, -8.2080e+00, -9.3880e+00, -6.4663e+00,\n",
      "          -3.0707e+00, -3.3914e+00, -8.5459e+00, -4.2488e+00, -1.9966e+00,\n",
      "          -1.6208e+00, -6.3082e+00, -8.2080e+00, -9.3880e+00, -6.4663e+00]],\n",
      "\n",
      "        [[-6.9900e+00, -1.2852e-01, -1.6499e+01, -4.3283e-02, -6.4308e+00,\n",
      "          -8.0380e-01, -8.3712e+00, -3.8242e-04, -1.2721e+01, -8.0147e-03,\n",
      "          -6.9900e+00, -1.2852e-01, -1.6499e+01, -4.3283e-02, -6.4308e+00,\n",
      "          -8.0380e-01, -8.3712e+00, -3.8242e-04, -1.2721e+01, -8.0147e-03],\n",
      "         [-6.9900e+00, -1.2852e-01, -1.6499e+01, -4.3283e-02, -6.4308e+00,\n",
      "          -8.0380e-01, -8.3712e+00, -3.8242e-04, -1.2721e+01, -8.0147e-03,\n",
      "          -6.9900e+00, -1.2852e-01, -1.6499e+01, -4.3283e-02, -6.4308e+00,\n",
      "          -8.0380e-01, -8.3712e+00, -3.8242e-04, -1.2721e+01, -8.0147e-03],\n",
      "         [-6.9900e+00, -1.2852e-01, -1.6499e+01, -4.3283e-02, -6.4308e+00,\n",
      "          -8.0380e-01, -8.3712e+00, -3.8242e-04, -1.2721e+01, -8.0147e-03,\n",
      "          -6.9900e+00, -1.2852e-01, -1.6499e+01, -4.3283e-02, -6.4308e+00,\n",
      "          -8.0380e-01, -8.3712e+00, -3.8242e-04, -1.2721e+01, -8.0147e-03],\n",
      "         [-6.9900e+00, -1.2852e-01, -1.6499e+01, -4.3283e-02, -6.4308e+00,\n",
      "          -8.0380e-01, -8.3712e+00, -3.8242e-04, -1.2721e+01, -8.0147e-03,\n",
      "          -6.9900e+00, -1.2852e-01, -1.6499e+01, -4.3283e-02, -6.4308e+00,\n",
      "          -8.0380e-01, -8.3712e+00, -3.8242e-04, -1.2721e+01, -8.0147e-03],\n",
      "         [-6.9900e+00, -1.2852e-01, -1.6499e+01, -4.3283e-02, -6.4308e+00,\n",
      "          -8.0380e-01, -8.3712e+00, -3.8242e-04, -1.2721e+01, -8.0147e-03,\n",
      "          -6.9900e+00, -1.2852e-01, -1.6499e+01, -4.3283e-02, -6.4308e+00,\n",
      "          -8.0380e-01, -8.3712e+00, -3.8242e-04, -1.2721e+01, -8.0147e-03],\n",
      "         [-6.9900e+00, -1.2852e-01, -1.6499e+01, -4.3283e-02, -6.4308e+00,\n",
      "          -8.0380e-01, -8.3712e+00, -3.8242e-04, -1.2721e+01, -8.0147e-03,\n",
      "          -6.9900e+00, -1.2852e-01, -1.6499e+01, -4.3283e-02, -6.4308e+00,\n",
      "          -8.0380e-01, -8.3712e+00, -3.8242e-04, -1.2721e+01, -8.0147e-03],\n",
      "         [-6.9900e+00, -1.2852e-01, -1.6499e+01, -4.3283e-02, -6.4308e+00,\n",
      "          -8.0380e-01, -8.3712e+00, -3.8242e-04, -1.2721e+01, -8.0147e-03,\n",
      "          -6.9900e+00, -1.2852e-01, -1.6499e+01, -4.3283e-02, -6.4308e+00,\n",
      "          -8.0380e-01, -8.3712e+00, -3.8242e-04, -1.2721e+01, -8.0147e-03],\n",
      "         [-6.9900e+00, -1.2852e-01, -1.6499e+01, -4.3283e-02, -6.4308e+00,\n",
      "          -8.0380e-01, -8.3712e+00, -3.8242e-04, -1.2721e+01, -8.0147e-03,\n",
      "          -6.9900e+00, -1.2852e-01, -1.6499e+01, -4.3283e-02, -6.4308e+00,\n",
      "          -8.0380e-01, -8.3712e+00, -3.8242e-04, -1.2721e+01, -8.0147e-03],\n",
      "         [-6.9900e+00, -1.2852e-01, -1.6499e+01, -4.3283e-02, -6.4308e+00,\n",
      "          -8.0380e-01, -8.3712e+00, -3.8242e-04, -1.2721e+01, -8.0147e-03,\n",
      "          -6.9900e+00, -1.2852e-01, -1.6499e+01, -4.3283e-02, -6.4308e+00,\n",
      "          -8.0380e-01, -8.3712e+00, -3.8242e-04, -1.2721e+01, -8.0147e-03],\n",
      "         [-6.9900e+00, -1.2852e-01, -1.6499e+01, -4.3283e-02, -6.4308e+00,\n",
      "          -8.0380e-01, -8.3712e+00, -3.8242e-04, -1.2721e+01, -8.0147e-03,\n",
      "          -6.9900e+00, -1.2852e-01, -1.6499e+01, -4.3283e-02, -6.4308e+00,\n",
      "          -8.0380e-01, -8.3712e+00, -3.8242e-04, -1.2721e+01, -8.0147e-03]],\n",
      "\n",
      "        [[-6.4699e-02, -7.4327e+00, -1.9836e-04, -1.2750e+01, -1.5574e-01,\n",
      "          -2.4568e+00, -4.4355e-03, -2.2526e+01, -2.1935e-04, -1.3618e+01,\n",
      "          -6.4699e-02, -7.4327e+00, -1.9836e-04, -1.2750e+01, -1.5574e-01,\n",
      "          -2.4568e+00, -4.4355e-03, -2.2526e+01, -2.1935e-04, -1.3618e+01],\n",
      "         [-6.4699e-02, -7.4327e+00, -1.9836e-04, -1.2750e+01, -1.5574e-01,\n",
      "          -2.4568e+00, -4.4355e-03, -2.2526e+01, -2.1935e-04, -1.3618e+01,\n",
      "          -6.4699e-02, -7.4327e+00, -1.9836e-04, -1.2750e+01, -1.5574e-01,\n",
      "          -2.4568e+00, -4.4355e-03, -2.2526e+01, -2.1935e-04, -1.3618e+01],\n",
      "         [-6.4699e-02, -7.4327e+00, -1.9836e-04, -1.2750e+01, -1.5574e-01,\n",
      "          -2.4568e+00, -4.4355e-03, -2.2526e+01, -2.1935e-04, -1.3618e+01,\n",
      "          -6.4699e-02, -7.4327e+00, -1.9836e-04, -1.2750e+01, -1.5574e-01,\n",
      "          -2.4568e+00, -4.4355e-03, -2.2526e+01, -2.1935e-04, -1.3618e+01],\n",
      "         [-6.4699e-02, -7.4327e+00, -1.9836e-04, -1.2750e+01, -1.5574e-01,\n",
      "          -2.4568e+00, -4.4355e-03, -2.2526e+01, -2.1935e-04, -1.3618e+01,\n",
      "          -6.4699e-02, -7.4327e+00, -1.9836e-04, -1.2750e+01, -1.5574e-01,\n",
      "          -2.4568e+00, -4.4355e-03, -2.2526e+01, -2.1935e-04, -1.3618e+01],\n",
      "         [-6.4699e-02, -7.4327e+00, -1.9836e-04, -1.2750e+01, -1.5574e-01,\n",
      "          -2.4568e+00, -4.4355e-03, -2.2526e+01, -2.1935e-04, -1.3618e+01,\n",
      "          -6.4699e-02, -7.4327e+00, -1.9836e-04, -1.2750e+01, -1.5574e-01,\n",
      "          -2.4568e+00, -4.4355e-03, -2.2526e+01, -2.1935e-04, -1.3618e+01],\n",
      "         [-6.4699e-02, -7.4327e+00, -1.9836e-04, -1.2750e+01, -1.5574e-01,\n",
      "          -2.4568e+00, -4.4355e-03, -2.2526e+01, -2.1935e-04, -1.3618e+01,\n",
      "          -6.4699e-02, -7.4327e+00, -1.9836e-04, -1.2750e+01, -1.5574e-01,\n",
      "          -2.4568e+00, -4.4355e-03, -2.2526e+01, -2.1935e-04, -1.3618e+01],\n",
      "         [-6.4699e-02, -7.4327e+00, -1.9836e-04, -1.2750e+01, -1.5574e-01,\n",
      "          -2.4568e+00, -4.4355e-03, -2.2526e+01, -2.1935e-04, -1.3618e+01,\n",
      "          -6.4699e-02, -7.4327e+00, -1.9836e-04, -1.2750e+01, -1.5574e-01,\n",
      "          -2.4568e+00, -4.4355e-03, -2.2526e+01, -2.1935e-04, -1.3618e+01],\n",
      "         [-6.4699e-02, -7.4327e+00, -1.9836e-04, -1.2750e+01, -1.5574e-01,\n",
      "          -2.4568e+00, -4.4355e-03, -2.2526e+01, -2.1935e-04, -1.3618e+01,\n",
      "          -6.4699e-02, -7.4327e+00, -1.9836e-04, -1.2750e+01, -1.5574e-01,\n",
      "          -2.4568e+00, -4.4355e-03, -2.2526e+01, -2.1935e-04, -1.3618e+01],\n",
      "         [-6.4699e-02, -7.4327e+00, -1.9836e-04, -1.2750e+01, -1.5574e-01,\n",
      "          -2.4568e+00, -4.4355e-03, -2.2526e+01, -2.1935e-04, -1.3618e+01,\n",
      "          -6.4699e-02, -7.4327e+00, -1.9836e-04, -1.2750e+01, -1.5574e-01,\n",
      "          -2.4568e+00, -4.4355e-03, -2.2526e+01, -2.1935e-04, -1.3618e+01],\n",
      "         [-6.4699e-02, -7.4327e+00, -1.9836e-04, -1.2750e+01, -1.5574e-01,\n",
      "          -2.4568e+00, -4.4355e-03, -2.2526e+01, -2.1935e-04, -1.3618e+01,\n",
      "          -6.4699e-02, -7.4327e+00, -1.9836e-04, -1.2750e+01, -1.5574e-01,\n",
      "          -2.4568e+00, -4.4355e-03, -2.2526e+01, -2.1935e-04, -1.3618e+01]]], device='cuda:0')\n",
      "tensor([[-1.3133, -1.3133, -1.3133, -1.3133, -1.3133, -1.3133, -1.3133,\n",
      "         -1.3133, -1.3133, -1.3133],\n",
      "        [-1.6208, -1.6208, -1.6208, -1.6208, -1.6208, -1.6208, -1.6208,\n",
      "         -1.6208, -1.6208, -1.6208],\n",
      "        [-0.0004, -0.0004, -0.0004, -0.0004, -0.0004, -0.0004, -0.0004,\n",
      "         -0.0004, -0.0004, -0.0004],\n",
      "        [-0.0002, -0.0002, -0.0002, -0.0002, -0.0002, -0.0002, -0.0002,\n",
      "         -0.0002, -0.0002, -0.0002]], device='cuda:0')\n",
      "tensor([ 7,  5,  6,  6])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adityasan92/anaconda2/envs/pytorch/lib/python3.5/site-packages/ipykernel_launcher.py:47: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, (data, label) in enumerate(train_loader):\n",
    "    print(data.shape)\n",
    "    print(gs.forward(data.to(device).type(torch.float)))\n",
    "    print(label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def denormalize( T, coords):\n",
    "    return (0.5 * ((coords + 1.0) * T) - 0.1).long()\n",
    "denormalize(torch.tensor(1),28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
